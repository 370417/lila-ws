http.port = 9664
mongo.uri = "mongodb://localhost:27017/lichess"
redis.uri = "redis://127.0.0.1"
csrf.origin = "http://l.org"
# csrf.origin = "https://lichess.org"

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-config-on-start = off
  actor {
    default-dispatcher {
      # Must be one of the following
      # Dispatcher, PinnedDispatcher, or a FQCN to a class inheriting
      # MessageDispatcherConfigurator with a public constructor with
      # both com.typesafe.config.Config parameter and
      # akka.dispatch.DispatcherPrerequisites parameters.
      # PinnedDispatcher must be used together with executor=thread-pool-executor.
      type = "Dispatcher"

      # Which kind of ExecutorService to use for this dispatcher
      # Valid options:
      #  - "default-executor" requires a "default-executor" section
      #  - "fork-join-executor" requires a "fork-join-executor" section
      #  - "thread-pool-executor" requires a "thread-pool-executor" section
      #  - "affinity-pool-executor" requires an "affinity-pool-executor" section
      #  - A FQCN of a class extending ExecutorServiceConfigurator
      executor = "default-executor"

      # This will be used if you have set "executor = "default-executor"".
      # If an ActorSystem is created with a given ExecutionContext, this
      # ExecutionContext will be used as the default executor for all
      # dispatchers in the ActorSystem configured with
      # executor = "default-executor". Note that "default-executor"
      # is the default value for executor, and therefore used if not
      # specified otherwise. If no ExecutionContext is given,
      # the executor configured in "fallback" will be used.
      default-executor {
        fallback = "fork-join-executor"
      }

      # This will be used if you have set "executor = "fork-join-executor""
      # Underlying thread pool implementation is java.util.concurrent.ForkJoinPool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 1

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 1.0

        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 32

        # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
        # like peeking mode which "pop".
        task-peeking-mode = "FIFO"
      }

      # This will be used if you have set "executor = "thread-pool-executor""
      # Underlying thread pool implementation is java.util.concurrent.ThreadPoolExecutor
      thread-pool-executor {
        # Keep alive time for threads
        keep-alive-time = 60s

        # Define a fixed thread pool size with this property. The corePoolSize
        # and the maximumPoolSize of the ThreadPoolExecutor will be set to this
        # value, if it is defined. Then the other pool-size properties will not
        # be used.
        #
        # Valid values are: `off` or a positive integer.
        fixed-pool-size = off

        # Min number of threads to cap factor-based corePoolSize number to
        core-pool-size-min = 8

        # The core-pool-size-factor is used to determine corePoolSize of the
        # ThreadPoolExecutor using the following formula:
        # ceil(available processors * factor).
        # Resulting size is then bounded by the core-pool-size-min and
        # core-pool-size-max values.
        core-pool-size-factor = 3.0

        # Max number of threads to cap factor-based corePoolSize number to
        core-pool-size-max = 64

        # Minimum number of threads to cap factor-based maximumPoolSize number to
        max-pool-size-min = 8

        # The max-pool-size-factor is used to determine maximumPoolSize of the
        # ThreadPoolExecutor using the following formula:
        # ceil(available processors * factor)
        # The maximumPoolSize will not be less than corePoolSize.
        # It is only used if using a bounded task queue.
        max-pool-size-factor  = 3.0

        # Max number of threads to cap factor-based maximumPoolSize number to
        max-pool-size-max = 64

        # Specifies the bounded capacity of the task queue (< 1 == unbounded)
        task-queue-size = -1

        # Specifies which type of task queue will be used, can be "array" or
        # "linked" (default)
        task-queue-type = "linked"

        # Allow core threads to time out
        allow-core-timeout = on
      }

      # How long time the dispatcher will wait for new actors until it shuts down
      shutdown-timeout = 1s

      # Throughput defines the number of messages that are processed in a batch
      # before the thread is returned to the pool. Set to 1 for as fair as possible.
      throughput = 5

      # Throughput deadline for Dispatcher, set to 0 or negative for no deadline
      throughput-deadline-time = 0ms

      # For BalancingDispatcher: If the balancing dispatcher should attempt to
      # schedule idle actors using the same dispatcher when a message comes in,
      # and the dispatchers ExecutorService is not fully busy already.
      attempt-teamwork = on

      # If this dispatcher requires a specific type of mailbox, specify the
      # fully-qualified class name here; the actually created mailbox will
      # be a subtype of this type. The empty string signifies no requirement.
      mailbox-requirement = ""
    }
  }
}

play {
  server {
    websocket {
      # Maximum allowable frame payload length. Setting this value to your application's
      # requirement may reduce denial of service attacks using long data frames.
      # frame.maxLength = 64k
      frame.maxLength = 4k
    }
    provider: play.core.server.NettyServerProvider
    netty {
      transport = native
    }
  }
  filters {
  }
}

play.http.secret.key="cTsuAGrQFNTBeoKJseg5jRUNmA1oFimrOPPjWHUKQKfwBJtrM85kdt2KwzPfEN0Q"

kamon {

  environment {
    service = "lila-ws"
  }

  instrumentation.akka.filters {

    # actors.track {
    #   includes = [ "clients/user/**" ]
    # }

    # dispatchers {
    #   includes = [ "my-app/akka.actor.default-dispatcher" ]
    # }

    # routers {
    #   includes = [ "my-app/user/some-router" ]
    # }
  }

  influxdb {

    # Hostname and port in which your InfluxDB is running
    hostname = ""
    port = 8086

    # The database where to write in InfluxDB.
    database = "kamon"

    # For histograms, which percentiles to count
    percentiles = [50.0, 70.0, 90.0, 95.0, 99.0, 99.9]

    # The protocol to use when used to connect to your InfluxDB: HTTP/HTTPS
    protocol = "http"

    # Whether or not to submit distributions with count = 0 to influxdb
    # (with 0 values)
    post-empty-distributions = false

    # The precision to report the period timestamp in. Corresponds with
    # what influx will accept, minus hours and minutes [ns,u,Âµ,ms,s]
    precision = "s"

    # Allow including environment information as tags on all reported metrics.
    environment-tags {

      # Define whether specific environment settings will be included as tags
      # in all exposed metrics. When enabled, the service, host and instance
      # tags will be added using the values from Kamon.environment().
      include-service = yes
      include-host = yes
      include-instance = yes

      # Specifies which Kamon environment tags should be ignored. All unmatched
      # tags will be always added to al metrics.
      exclude = []
    }

    tag-filter {
      includes = ["**"]
      excludes = []
    }
  }

  modules {
    jvm-metrics {
      enabled = yes
    }
    process-metrics {
      enabled = yes
    }
    host-metrics {
      enabled = no
    }
  }
}
